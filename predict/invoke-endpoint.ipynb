{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'sagemaker-eu-west-1-366243680492'\n",
    "s3_file = 'data-processing-2020-06-12-08-37-01-419/output/preprocessed/MRPC/train.tsv'\n",
    "\n",
    "s3.download_file(bucket, s3_file, 'train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "endpoint_name = 'training-pipeline-2020-06-12-10-23-45'\n",
    "\n",
    "class Predictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer, \n",
    "                         deserializer=json_deserializer, content_type='application/json')\n",
    "\n",
    "bert_predictor = Predictor(endpoint_name, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.tsv', sep='\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = data['#1 String'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = bert_predictor.predict(item)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "modelpath = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(modelpath)\n",
    "\n",
    "text = \"dummy. although he had already eaten a large meal, he was still very hungry.\"\n",
    "target = \"hungry\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = tokenized_text.index(target)\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "# this is for the dummy first sentence. \n",
    "segments_ids[0] = 0\n",
    "segments_ids[1] = 0\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained(modelpath)\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "predictions = model(tokens_tensor, segments_tensors)\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Masked:\", \" \".join(tokenized_text))\n",
    "\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "print(\"Other options:\")\n",
    "# just curious about what the next few options look like.\n",
    "for i in range(10):\n",
    "    predictions[0,masked_index,predicted_index] = -11100000\n",
    "    predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "    print(predicted_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
